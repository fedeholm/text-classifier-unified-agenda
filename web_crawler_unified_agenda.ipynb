{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgI3OViAkai2hHpwDYM/yU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fedeholm/text-classifier-unified-agenda/blob/main/web_crawler_unified_agenda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "cRGl8lXOmecr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import a csv file with hyperlinks ready to be read by a web crawler. This step is only to check that the file can be imported properly. This step is not needed, but can be useful when dealing with convoluted file paths\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "try:\n",
        "    data_links = pd.read_csv('all_years_rules_links.csv', encoding='latin-1') # The name of the file matches the name of the file in the repo.\n",
        "    print(\"CSV file loaded successfully.\")\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(data_links.head())\n",
        "\n",
        "except UnicodeDecodeError:\n",
        "    print(\"Error: Could not decode the file using 'latin-1' encoding. Please try another encoding like 'cp1252'.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Reg_agenda_links.csv' not found. Please make sure the file is in the correct directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "riHJHmyVx3nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Webcrawler to extract designations. This chunk streamlines the reading of the data and retrieves the EO designations for each rule\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "\n",
        "try:\n",
        "    data_links = pd.read_csv('all_years_rules_links.csv', encoding='latin-1')\n",
        "except UnicodeDecodeError:\n",
        "    print(\"Error: Could not decode the file using 'latin-1' encoding. Please try another encoding like 'cp1252'.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: file not found. Please make sure the file is in the correct directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "links_df = pd.DataFrame(data_links)\n",
        "\n",
        "\n",
        "# web crawler to capture the following 2 strings after \"EO 13771 Designation\" in the links explored\n",
        "\n",
        "def crawl_and_extract_designations(df, url_column='Link'):\n",
        "    \"\"\"\n",
        "    Crawls each URL in a DataFrame column, extracts the two strings following\n",
        "    \"EO 13771 Designation\", and returns a new DataFrame with the extracted strings.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing URLs.\n",
        "        url_column (str): The name of the column in the DataFrame containing URLs.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame with added and 'Designation_2' column.\n",
        "    \"\"\"\n",
        "    if url_column not in df.columns:\n",
        "        raise ValueError(f\"DataFrame does not have a column named '{url_column}'\")\n",
        "\n",
        "    # Initialize column to store the extracted strings\n",
        "    df['Designation_2'] = None\n",
        "\n",
        "    search_pattern = re.compile(r'EO 13771 Designation:\\s*(.*?)\\s*(.*)')\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        url = row[url_column]\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text()\n",
        "\n",
        "            # Search for the pattern in the text\n",
        "            match = search_pattern.search(text)\n",
        "\n",
        "            if match:\n",
        "                designation2 = match.group(2).strip()\n",
        "                df.loc[index, 'Designation_2'] = designation2\n",
        "                print(f\"Processed {url}: EO Designation: '{designation2}'\")\n",
        "            else:\n",
        "                print(f\"Processed {url}: 'EO 13771 Designation' not found or pattern mismatch.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error processing {url}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred while processing {url}: {e}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example Usage with the sample DataFrame defined previously:\n",
        "processed_df_designations = crawl_and_extract_designations(links_df, url_column='Link')\n",
        "\n",
        "#print(\"\\nDataFrame with extracted designations:\")\n",
        "#processed_df_designations"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1RB5kQZS1epz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export the new DataFrame to CSV\n",
        "processed_df_designations.to_csv('FILENAME.csv', index=False)"
      ],
      "metadata": {
        "id": "UL_3K6krX-em",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############# IF NEEDED - LOAD A NEW VERSION OF THE DF\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "\n",
        "try:\n",
        "    data_links = pd.read_csv('all_years_rules_links.csv', encoding='latin-1') # The name of the file matches the name of the file in the repo.\n",
        "except UnicodeDecodeError:\n",
        "    print(\"Error: Could not decode the file using 'latin-1' encoding. Please try another encoding like 'cp1252'.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Reg_agenda_links.csv' not found. Please make sure the file is in the correct directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "links_df = pd.DataFrame(data_links)"
      ],
      "metadata": {
        "id": "ugVKbsPmsY1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Webcrawler to extract abstracts\n",
        "\n",
        "# web crawler to capture the following 2 strings after \"Abstract\" in the links\n",
        "# explored\n",
        "\n",
        "def crawl_and_extract_designations(df, url_column='Link'):\n",
        "    \"\"\"\n",
        "    Crawls each URL in a DataFrame column, extracts the two strings following\n",
        "    \"Abstract\", and returns a new DataFrame with the extracted strings.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing URLs.\n",
        "        url_column (str): The name of the column in the DataFrame containing URLs.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame with added 'Abstract' column.\n",
        "    \"\"\"\n",
        "    if url_column not in df.columns:\n",
        "        raise ValueError(f\"DataFrame does not have a column named '{url_column}'\")\n",
        "\n",
        "    # Initialize columns to store the extracted strings\n",
        "\n",
        "    df['Abstract'] = None\n",
        "\n",
        "    search_pattern = re.compile(r'Abstract:\\s*(.*?)\\s*(.*)')\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        url = row[url_column]\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text()\n",
        "\n",
        "            # Search for the pattern in the text\n",
        "            match = search_pattern.search(text)\n",
        "\n",
        "            if match:\n",
        "                abstract2 = match.group(2).strip()\n",
        "                df.loc[index, 'Abstract'] = abstract2\n",
        "                print(f\"Processed {url}: Abstract: '{abstract2}'\")\n",
        "            else:\n",
        "                print(f\"Processed {url}: 'Abstract' not found or pattern mismatch.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error processing {url}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred while processing {url}: {e}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example Usage with the sample DataFrame defined previously:\n",
        "processed_df_abstracts = crawl_and_extract_designations(links_df, url_column='Link')\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OPmvh2OeNspl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export the new DataFrame to CSV\n",
        "processed_df_abstracts.to_csv('FILENAME.csv', index=False)"
      ],
      "metadata": {
        "id": "14krgde7OntC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we import the necessary packages and start coding our text classifier."
      ],
      "metadata": {
        "id": "1N47sYZjIR1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "xZroSaL9HdTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"TrainTest_designations_all_years.csv\")\n",
        "X = df['Abstract']\n",
        "y = df['Designation']"
      ],
      "metadata": {
        "id": "pNgyw4fSIee2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Designation'].value_counts()"
      ],
      "metadata": {
        "id": "oDKUKmJ_HRKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting our dataset into training and testing sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=57)\n"
      ],
      "metadata": {
        "id": "RSTM9IfwI8-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use the scikit-learn pipeline which is just a sequence of steps to take\n",
        "# to build our classifier. In this pipeline, we convert the data to a numerical\n",
        "# format using TfidfVectorizer and then specify our classifiers\n",
        "\n",
        "pipeMNB = Pipeline([('tfidf', TfidfVectorizer()),('clf', MultinomialNB())])\n",
        "pipeCNB = Pipeline([('tfidf', TfidfVectorizer()),('clf', ComplementNB())])\n",
        "pipeSVC = Pipeline([('tfidf', TfidfVectorizer()),('clf', LinearSVC())])"
      ],
      "metadata": {
        "id": "Ws5QNWisKrX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can try to optimize the best performing models with additional arguments\n",
        "\n",
        "pipeMNB = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1,3))),('clf', MultinomialNB())])\n",
        "pipeCNB = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1,3))),('clf', ComplementNB())])\n",
        "pipeSVC = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1,3))),('clf', LinearSVC())])"
      ],
      "metadata": {
        "id": "Dj7p0cxKLHac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will build our models using MultinomialNB, ComplementNB, and LinearSVC,\n",
        "# and training it (fitting) on our train data. After that, we predict the labels\n",
        "# for our TEST data, and then we print out the accuracy score based on a\n",
        "# comparison of the correct labels and our predictions\n",
        "\n",
        "pipeMNB.fit(X_train, y_train)\n",
        "predictMNB = pipeMNB.predict(X_test)\n",
        "print(f\"MNB: {accuracy_score(y_test, predictMNB):.2f}\")\n",
        "\n",
        "pipeCNB.fit(X_train, y_train)\n",
        "predictCNB = pipeCNB.predict(X_test)\n",
        "print(f\"CNB: {accuracy_score(y_test, predictCNB):.2f}\")\n"
      ],
      "metadata": {
        "id": "4EF_upLKK9hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best performing so far\n",
        "pipeSVC.fit(X_train, y_train)\n",
        "predictSVC = pipeSVC.predict(X_test)\n",
        "print(f\"SVC: {accuracy_score(y_test, predictSVC):.2f}\")"
      ],
      "metadata": {
        "id": "rV3vLXlJRnR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see how the classifeir performs in a new piece of text\n",
        "\n",
        "# noticed that 'new' 'updated' is attached to 'dereg'.\n",
        "\n",
        "abstract = \"This rule will safeguard health of outdoor workers\"\n",
        "result = pipeSVC.predict([abstract])\n",
        "print(\"Result: \", result[0])"
      ],
      "metadata": {
        "id": "OrDEeKxEMRG1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}